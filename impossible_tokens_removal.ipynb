{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Impossible Tokens\n",
    "\n",
    "- words are being tokenized into different tokens using different tokenizer. In order to help the model learn better, we removed tokens that are impossible to predict so that we don't penalize them during loss calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhCKPIiifHJd"
   },
   "outputs": [],
   "source": [
    "def preprocess_im(df):\n",
    "    im_dict = {\n",
    "        'iï¿½m':\"i'm\",\n",
    "        'Iï¿½m':\"I'm\",\n",
    "        'Iï¿½M':\"I'M\",\n",
    "        'Iï¿½d':\"I'd\",\n",
    "        'Iï¿½D':\"I'D\",\n",
    "    }\n",
    "    \n",
    "    for key, item in im_dict.items():\n",
    "        \n",
    "        df.loc[df['text'].str.contains(key),'selected_text'] = df.loc[\n",
    "                    df['text'].str.contains(key),'selected_text'\n",
    "        ].apply(lambda x: re.sub(key, item, x))\n",
    "        \n",
    "        \n",
    "        df.loc[df['text'].str.contains(key),'text'] = df.loc[\n",
    "                    df['text'].str.contains(key),'text'\n",
    "        ].apply(lambda x: re.sub(key, item, x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_all(df):\n",
    "    \n",
    "    df.loc[:, 'text'] = df.loc[:, 'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:, 'selected_text'] = df.loc[:, 'selected_text'].apply(lambda x: x.lower())\n",
    "    \n",
    "    proc_dict = {\n",
    "        'ï¿½s':\"'s\",\n",
    "        'nï¿½t':\"n't\",\n",
    "        'ï¿½ve':\"'ve\",\n",
    "        'ï¿½ll':\"'ll\",\n",
    "        'ï¿½re':\"'re\",\n",
    "        \"inï¿½\": \"ing\",\n",
    "        \"n`\\*\\*\\*\\*\": \"n't\"\n",
    "    }\n",
    "    for key, item in proc_dict.items():\n",
    "        if key == '`s':\n",
    "            df.loc[df['text'].str.contains(\"(\\w`s)\"),'selected_text'] = df.loc[\n",
    "                df['text'].str.contains(\"(\\w`s)\"),'selected_text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "            df.loc[(df['text'].str.contains(\"(\\w`s)\")),'text'] = df.loc[\n",
    "                df['text'].str.contains(\"(\\w`s)\"),'text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "        else:\n",
    "            df.loc[df['text'].str.contains(key),'selected_text'] = df.loc[\n",
    "                df['text'].str.contains(key),'selected_text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "            df.loc[df['text'].str.contains(key),'text'] = df.loc[\n",
    "                df['text'].str.contains(key),'text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "    \n",
    "    df.loc[df['selected_text'].str.contains(\"(ï|¿|½)\"),'selected_text'] = df.loc[\n",
    "        df['selected_text'].str.contains(\"(ï|¿|½)\"),'selected_text'\n",
    "    ].apply(lambda x: re.sub(\"(ï|¿|½)\", \"\", x))\n",
    "    \n",
    "    \n",
    "    df.loc[df['text'].str.contains(\"(ï|¿|½)\"),'text'] = df.loc[\n",
    "        df['text'].str.contains(\"(ï|¿|½)\"),'text'\n",
    "    ].apply(lambda x: re.sub(\"(ï|¿|½)\", \"\", x))\n",
    "\n",
    "            \n",
    "    return df\n",
    "\n",
    "def preprocess_repeat(df):\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\.)(\\.)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\.)(\\.)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\!)(\\!)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\!)(\\!)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\?)(\\?)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\?)(\\?)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aP5LVRtd0xrW"
   },
   "outputs": [],
   "source": [
    "def token_encode(x):\n",
    "    encoded = TOKENIZER.encode(x)\n",
    "    offsets = encoded.offsets\n",
    "    # ids = encoded.ids\n",
    "    # return [offset for i,offset in enumerate(offsets) if ids[i]!=47341]\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 192\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "ROBERTA_PATH = \"./roberta-base-squad2/\"\n",
    "TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
    "    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Start token problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vg-YRENiSoDB"
   },
   "outputs": [],
   "source": [
    "def check_start(offsets, span_start, text):\n",
    "    for i,j in offsets:\n",
    "        if i==span_start:\n",
    "            return 0\n",
    "        k=i\n",
    "        while len(text)-1 > k and text[k]==' ':\n",
    "        # while text[k]==' ':\n",
    "            if text[k]==' ':\n",
    "                k=k+1\n",
    "            else: break\n",
    "        if k==span_start:\n",
    "            return 0\n",
    "        if k>span_start:\n",
    "            return j-span_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "7HURbq_MTO0p",
    "outputId": "c56c742e-f2e1-48ae-b382-28093549f79a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     26736\n",
      "6       140\n",
      "7       120\n",
      "5        98\n",
      "8        74\n",
      "4        71\n",
      "3        65\n",
      "2        57\n",
      "9        55\n",
      "10       26\n",
      "11       22\n",
      "12        7\n",
      "13        6\n",
      "15        2\n",
      "14        1\n",
      "Name: is_start_problem, dtype: int64\n",
      "0    27464\n",
      "7        1\n",
      "Name: is_start_problem, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('train_folds_42.csv')\n",
    "# dat = dat[dat['sentiment']=='neutral'].reset_index(drop=True)\n",
    "\n",
    "dat['text'] = dat['text'].str.strip()\n",
    "dat['selected_text'] = dat['selected_text'].str.strip()\n",
    "dat['text'] = dat['text'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "dat['selected_text'] = dat['selected_text'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "\n",
    "# dat = preprocess_im(dat)\n",
    "# dat = preprocess_all(dat)\n",
    "# dat = preprocess_repeat(dat)\n",
    "\n",
    "dat['offsets'] = dat['text'].map(\n",
    "    lambda x: token_encode(x)\n",
    ")\n",
    "dat['span'] = dat.apply(\n",
    "    lambda x: re.search(re.escape(x['selected_text']), x['text']).span() , axis=1\n",
    ")\n",
    "dat['is_start_problem'] = dat.apply(lambda x: check_start(x['offsets'], x['span'][0], x['text']), axis=1)\n",
    "print(dat['is_start_problem'].value_counts())\n",
    "dat['selected_2'] = dat['selected_text']\n",
    "dat.loc[dat['is_start_problem']>0,'selected_2'] = dat.loc[dat['is_start_problem']>0,'selected_text'].map(\n",
    "    lambda x: ' '.join(x.split()[1:])\n",
    ").str.strip()\n",
    "dat['text'] = dat['text'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "dat['offsets'] = dat['text'].map(\n",
    "    lambda x: token_encode(x)\n",
    ")\n",
    "dat['selected_2'] = dat['selected_2'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "dat['selected_2'] = dat['selected_2'].map(lambda x: re.sub('^\\s+$','',x)).replace('',np.nan)\n",
    "dat = dat.dropna().reset_index(drop=True)\n",
    "dat.loc[dat['selected_2'] == '','selected_2'] = dat.loc[dat['selected_2'] == '','selected_text']\n",
    "dat['span'] = dat.apply(\n",
    "    lambda x: re.search(re.escape(x['selected_2']), x['text']).span(), axis=1\n",
    ")\n",
    "dat['is_start_problem'] = dat.apply(lambda x: check_start(x['offsets'], x['span'][0], x['text']), axis=1)\n",
    "dat = dat.loc[dat['selected_text'].map(len)>=2].reset_index(drop=True)\n",
    "# dat.loc[dat['selected_text'].map(len)<=1,'selected_2'] = dat.loc[dat['selected_text'].map(len)<=1,'selected_text']\n",
    "print(dat['is_start_problem'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check end token problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOSFRBLNqRX_"
   },
   "outputs": [],
   "source": [
    "def check_end(offsets, span_end, text):\n",
    "    for i,j in offsets:\n",
    "        k=j\n",
    "        while len(text)-1 > k and text[k] == '、':\n",
    "            k=k+1\n",
    "        if k==span_end:\n",
    "            return 0\n",
    "        if k>span_end:\n",
    "            return j-span_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "02xzuZG5yGyp",
    "outputId": "0bfc6df8-d08f-4d55-ec9a-e37d04167ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    26803\n",
      "1      372\n",
      "2      154\n",
      "5       45\n",
      "3       45\n",
      "4       33\n",
      "6        9\n",
      "8        2\n",
      "7        2\n",
      "Name: is_end_problem, dtype: int64\n",
      "0    27372\n",
      "1        5\n",
      "2        3\n",
      "Name: is_end_problem, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dat['is_end_problem'] = dat.apply(lambda x: check_end(x['offsets'], x['span'][1], x['text']), axis=1)\n",
    "print(dat['is_end_problem'].value_counts())\n",
    "dat['selected_3'] = dat['selected_2']\n",
    "dat.loc[dat['is_end_problem']>0,'selected_3'] = dat.loc[dat['is_end_problem']>0,'selected_2'].map(\n",
    "    lambda x: ' '.join(x.split()[:-1])\n",
    ").str.strip()\n",
    "dat['text'] = dat['text'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "dat['offsets'] = dat['text'].map(\n",
    "    lambda x: token_encode(x)\n",
    ")\n",
    "dat['selected_3'] = dat['selected_3'].map(lambda x: re.sub('\\s+',' ',x))\n",
    "dat['selected_3'] = dat['selected_3'].map(lambda x: re.sub('^\\s+$','',x)).replace('',np.nan)\n",
    "dat = dat.dropna().reset_index(drop=True)\n",
    "# dat.loc[dat['selected_3'] == '','selected_3'] = dat.loc[dat['selected_3'] == '','selected_2']\n",
    "dat.loc[dat['selected_text'].map(len)<=2,'selected_3'] = dat.loc[dat['selected_text'].map(len)<=2,'selected_text']\n",
    "dat['span'] = dat.apply(\n",
    "    lambda x: re.search(re.escape(x['selected_3']), x['text']).span(), axis=1\n",
    ")\n",
    "dat['is_end_problem'] = dat.apply(lambda x: check_end(x['offsets'], x['span'][1], x['text']), axis=1)\n",
    "print(dat['is_end_problem'].value_counts())\n",
    "# dat = dat.loc[dat['is_end_problem']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLmEd7o-BTiG"
   },
   "outputs": [],
   "source": [
    "# dat = dat.loc[dat['is_end_problem']==0].reset_index(drop=True)\n",
    "dat['selected_text'] = dat['selected_3']\n",
    "dat = dat.drop(['offsets','span','is_start_problem','selected_2','is_end_problem','selected_3'],1)\n",
    "dat.to_pickle('train_folds_42_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "impossible_tokens.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
