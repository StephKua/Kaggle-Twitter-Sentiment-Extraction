{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)\n",
    "import re\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 192\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "ROBERTA_PATH = \"./roberta-base-squad2/\"\n",
    "TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
    "    merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Postprocessing\n",
    "\n",
    "1. replace and retrieve unknown characters with relevant characters\n",
    "2. seperate tokens such as '!!!' or '???' into single token when encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_im(df):\n",
    "    im_dict = {\n",
    "        'iï¿½m':\"i'm\",\n",
    "        'Iï¿½m':\"I'm\",\n",
    "        'Iï¿½M':\"I'M\",\n",
    "        'Iï¿½d':\"I'd\",\n",
    "        'Iï¿½D':\"I'D\",\n",
    "    }\n",
    "    \n",
    "    for key, item in im_dict.items():\n",
    "        \n",
    "        df.loc[df['text'].str.contains(key),'selected_text'] = df.loc[\n",
    "                    df['text'].str.contains(key),'selected_text'\n",
    "        ].apply(lambda x: re.sub(key, item, x))\n",
    "        \n",
    "        \n",
    "        df.loc[df['text'].str.contains(key),'text'] = df.loc[\n",
    "                    df['text'].str.contains(key),'text'\n",
    "        ].apply(lambda x: re.sub(key, item, x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_all(df):\n",
    "    \n",
    "    df.loc[:, 'text'] = df.loc[:, 'text'].apply(lambda x: x.lower())\n",
    "    df.loc[:, 'selected_text'] = df.loc[:, 'selected_text'].apply(lambda x: x.lower())\n",
    "    \n",
    "    proc_dict = {\n",
    "        'ï¿½s':\"'s\",\n",
    "        'nï¿½t':\"n't\",\n",
    "        'ï¿½ve':\"'ve\",\n",
    "        'ï¿½ll':\"'ll\",\n",
    "        'ï¿½re':\"'re\",\n",
    "        \"inï¿½\": \"ing\",\n",
    "        \"n`\\*\\*\\*\\*\": \"n't\"\n",
    "    }\n",
    "    for key, item in proc_dict.items():\n",
    "        if key == '`s':\n",
    "            df.loc[df['text'].str.contains(\"(\\w`s)\"),'selected_text'] = df.loc[\n",
    "                df['text'].str.contains(\"(\\w`s)\"),'selected_text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "            df.loc[(df['text'].str.contains(\"(\\w`s)\")),'text'] = df.loc[\n",
    "                df['text'].str.contains(\"(\\w`s)\"),'text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "        else:\n",
    "            df.loc[df['text'].str.contains(key),'selected_text'] = df.loc[\n",
    "                df['text'].str.contains(key),'selected_text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "            \n",
    "            df.loc[df['text'].str.contains(key),'text'] = df.loc[\n",
    "                df['text'].str.contains(key),'text'\n",
    "            ].apply(lambda x: re.sub(key, item, x))\n",
    "    \n",
    "    df.loc[df['selected_text'].str.contains(\"((ï|¿|½))\"),'selected_text'] = df.loc[\n",
    "        df['selected_text'].str.contains(\"((ï|¿|½))\"),'selected_text'\n",
    "    ].apply(lambda x: re.sub(\"((ï|¿|½))\", \"\", x))\n",
    "    \n",
    "    \n",
    "    df.loc[df['text'].str.contains(\"((ï|¿|½))\"),'text'] = df.loc[\n",
    "        df['text'].str.contains(\"((ï|¿|½))\"),'text'\n",
    "    ].apply(lambda x: re.sub(\"((ï|¿|½))\", \"\", x))\n",
    "\n",
    "            \n",
    "    return df\n",
    "\n",
    "def preprocess_repeat(df):\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\.)(\\.)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\.)(\\.)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\!)(\\!)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\!)(\\!)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\"), 'selected_text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\")\n",
    "    ].selected_text.apply(lambda x:re.sub(r'(?<=\\?)(\\?)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    df.loc[df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\"), 'text'] = df.loc[\n",
    "        df.text.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\")\n",
    "    ].text.apply(lambda x:re.sub(r'(?<=\\?)(\\?)(?<!\\w)', r' \\1', x))\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def postprocess_repeat(df):\n",
    "    \n",
    "    df.loc[df.text_x.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\"), 'pred'] = df.loc[\n",
    "        df.text_x.str.contains(\"(?<=\\.)(\\.)(?<!\\w)\")\n",
    "    ].pred.apply(lambda x:re.sub(r'(?<=\\.)(\\s)(?=(\\.|\\s))', \"\", x))\n",
    "    \n",
    "    df.loc[df.text_x.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\"), 'pred'] = df.loc[\n",
    "        df.text_x.str.contains(\"(?<=\\!)(\\!)(?<!\\w)\")\n",
    "    ].pred.apply(lambda x:re.sub(r'(?<=\\!)(\\s)(?=(\\!|\\s))', \"\", x))\n",
    "    \n",
    "    df.loc[df.text_x.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\"), 'pred'] = df.loc[\n",
    "        df.text_x.str.contains(\"(?<=\\?)(\\?)(?<!\\w)\")\n",
    "    ].pred.apply(lambda x:re.sub(r'(?<=\\?)(\\s)(?=(\\?|\\s))', \"\", x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def postprocess_im(df):\n",
    "    im_dict = {\n",
    "        'iï¿½m':\"i'm\",\n",
    "        'Iï¿½m':\"I'm\",\n",
    "        'Iï¿½M':\"I'M\",\n",
    "        'Iï¿½d':\"I'd\",\n",
    "        'Iï¿½D':\"I'D\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for key, item in im_dict.items():\n",
    "        df.loc[df['text_x'].str.contains(key),'pred'] = df.loc[\n",
    "                    df['text_x'].str.contains(key),'pred'\n",
    "        ].apply(lambda x: re.sub(item.lower(), key, x))\n",
    "                \n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def postprocess_all(df):\n",
    "    \n",
    "    df.loc[:, 'text_x'] = df.loc[:, 'text_x'].apply(lambda x: x.lower())\n",
    "    df.loc[:, 'pred'] = df.loc[:, 'pred'].apply(lambda x: x.lower())\n",
    "    \n",
    "    proc_dict = {\n",
    "        'ï¿½s':\"'s\",\n",
    "        'nï¿½t':\"n't\",\n",
    "        'ï¿½ve':\"'ve\",\n",
    "        'ï¿½ll':\"'ll\",\n",
    "        'ï¿½re':\"'re\",\n",
    "        \"inï¿½\": \"ing\",\n",
    "        \"n`\\*\\*\\*\\*\": \"n't\"\n",
    "        \n",
    "    }\n",
    "    for key, item in proc_dict.items():\n",
    "        if key == '`s':\n",
    "            df.loc[df['text_x'].str.contains(\"(\\w`s)\"),'pred'] = df.loc[\n",
    "                df['text_x'].str.contains(\"(\\w`s)\"),'pred'\n",
    "            ].apply(lambda x: re.sub(item, key, x))\n",
    "        else:\n",
    "            df.loc[df['text_x'].str.contains(key),'pred'] = df.loc[\n",
    "                df['text_x'].str.contains(key),'pred'\n",
    "            ].apply(lambda x: re.sub(item, key, x))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "    \n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "    \n",
    "    if len_st == 0:\n",
    "        print(selected_text)\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "            \n",
    "#     processed_tweet = re.sub('、、、', '、', tweet)\n",
    "\n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    tweet_offsets = [offset for i, offset in enumerate(tok_tweet.offsets) if tok_tweet.ids[i]!=47341]\n",
    "    input_ids_orig = [id for id in tok_tweet.ids if id!=47341]\n",
    "\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "    if len(target_idx) == 0:\n",
    "        print(tweet, selected_text)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(0.5) for _ in range(5)\n",
    "        ])\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.avgpool = nn.AvgPool1d(4)\n",
    "        self.l0 = nn.Linear(768, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, _, out  = self.roberta(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        out = torch.cat((out[-1], out[-2], out[-3], out[-4]), dim=-1)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                h = self.l0(dropout(out))\n",
    "            else:\n",
    "                h += self.l0(dropout(out))\n",
    "        logits = h / len(self.dropouts)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_between(start_logits, end_logits, device='cuda', max_seq_len=192):\n",
    "    \"\"\"get dist btw. pred & ground_truth\"\"\"\n",
    "\n",
    "    linear_func = torch.tensor(np.linspace(0, 1, max_seq_len, endpoint=False), requires_grad=False)\n",
    "    linear_func = linear_func.to(device)\n",
    "\n",
    "    start_pos = (start_logits*linear_func).sum(axis=1)\n",
    "    end_pos = (end_logits*linear_func).sum(axis=1)\n",
    "\n",
    "    diff = end_pos-start_pos\n",
    "\n",
    "    return diff.sum(axis=0)/diff.size(0)\n",
    "\n",
    "\n",
    "def dist_loss(start_logits, end_logits, start_positions, end_positions, device='cuda', max_seq_len=192, scale=2):\n",
    "    \"\"\"calculate distance loss between prediction's length & GT's length\n",
    "    \n",
    "    Input\n",
    "    - start_logits ; shape (batch, max_seq_len{128})\n",
    "        - logits for start index\n",
    "    - end_logits\n",
    "        - logits for end index\n",
    "    - start_positions ; shape (batch, 1)\n",
    "        - start index for GT\n",
    "    - end_positions\n",
    "        - end index for GT\n",
    "    \"\"\"\n",
    "    start_logits = torch.nn.Softmax(1)(start_logits) # shape ; (batch, max_seq_len)\n",
    "    end_logits = torch.nn.Softmax(1)(end_logits)\n",
    "    \n",
    "    start_one_hot = torch.nn.functional.one_hot(start_positions, num_classes=max_seq_len).to(device)\n",
    "    end_one_hot = torch.nn.functional.one_hot(end_positions, num_classes=max_seq_len).to(device)\n",
    "    \n",
    "    pred_dist = dist_between(start_logits, end_logits, device, max_seq_len)\n",
    "    gt_dist = dist_between(start_one_hot, end_one_hot, device, max_seq_len) # always positive\n",
    "    diff = (gt_dist-pred_dist)\n",
    "\n",
    "    rev_diff_squared = 1-torch.sqrt(diff*diff) # as diff is smaller, make it get closer to the one\n",
    "    loss = -torch.log(rev_diff_squared) # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n",
    "\n",
    "    return loss*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    distance_loss = dist_loss(start_logits, end_logits, start_positions, end_positions)\n",
    "    total_loss = (start_loss + end_loss) + distance_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='max', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        return False\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    # if sentiment_val == \"neutral\":\n",
    "    #     filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch, d in enumerate(train_data_loader):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch % 300 == 0 and batch > 0:\n",
    "            print('Epoch: %d | %5d\\%5d batches | loss: %.5f' %\n",
    "                  (epoch + 1, batch, len(train_data_loader), running_loss / 300))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(valid_data_loader, model, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                jaccard_score, _  = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}, Loss = {losses.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_jaccard = []\n",
    "\n",
    "for fold in range(5):\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "    best_jaccard = 0\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "    es = EarlyStopping(patience=2)\n",
    "    es.step(0)\n",
    "\n",
    "    train = pd.read_pickle('train_folds_42_clean_steph_v1.pkl')\n",
    "    train = train.loc[(train.kfold != fold)].reset_index(drop=True)\n",
    "\n",
    "    train = preprocess_im(train)\n",
    "    train = preprocess_all(train)\n",
    "    train = preprocess_repeat(train)\n",
    "    \n",
    "\n",
    "    valid = pd.read_csv('train_folds_42.csv')\n",
    "    valid = valid.loc[(valid.kfold == fold)].reset_index(drop=True)\n",
    "\n",
    "    valid = preprocess_im(valid)\n",
    "    valid = preprocess_all(valid)\n",
    "    valid = preprocess_repeat(valid)\n",
    "    \n",
    "    \n",
    "    train_data = TweetDataset(tweet=train.text.values,\n",
    "                              sentiment=train.sentiment.values,\n",
    "                              selected_text=train.selected_text.values)\n",
    "\n",
    "    valid_data = TweetDataset(tweet=valid.text.values,\n",
    "                              sentiment=valid.sentiment.values,\n",
    "                              selected_text=valid.selected_text.values)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "                            train_data,\n",
    "                            shuffle=False,\n",
    "                            batch_size=TRAIN_BATCH_SIZE,\n",
    "                            num_workers=1\n",
    "                        )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "                            valid_data,\n",
    "                            shuffle=False,\n",
    "                            batch_size=VALID_BATCH_SIZE,\n",
    "                            num_workers=1\n",
    "                        )\n",
    "\n",
    "    num_train_steps = int(len(train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    optimizer = transformers.AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        jaccard_score = eval_fn(valid_data_loader, model, device)\n",
    "\n",
    "        if es.step(jaccard_score):\n",
    "            break\n",
    "        if jaccard_score > best_jaccard:\n",
    "            torch.save(model.state_dict(), f'Roberta_Local_{fold+1}.pth')\n",
    "            best_jaccard = jaccard_score\n",
    "\n",
    "    total_jaccard.append(best_jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_start_end_idxs(_start_logits, _end_logits):\n",
    "    best_logit = -1000\n",
    "    best_idxs = None\n",
    "    for start_idx, start_logit in enumerate(_start_logits):\n",
    "        addition = np.repeat(start_logit, len(_end_logits[start_idx:])) + _end_logits[start_idx:]\n",
    "        argmax = addition.argmax()\n",
    "        maximum = addition[addition.argmax()].item()\n",
    "        if maximum > best_logit:\n",
    "            best_logit = maximum\n",
    "            best_idxs = (start_idx, start_idx+argmax)\n",
    "    \n",
    "    return best_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_fn(data_loader, model, device):\n",
    "    final_output = []\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "    start_losses = []\n",
    "    end_losses = []\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "            start_loss = ce_loss(outputs_start, targets_start)\n",
    "            start_losses.append(start_loss.cpu().numpy())\n",
    "            end_loss = ce_loss(outputs_end, targets_end)\n",
    "            end_losses.append(end_loss.cpu().numpy())\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                best_start, best_end = get_best_start_end_idxs(outputs_start[px, :], outputs_end[px, :])\n",
    "                _, output_sentence = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=best_start,\n",
    "                    idx_end=best_end,\n",
    "#                     idx_start=np.argmax(outputs_start[px, :]),\n",
    "#                     idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                final_output.append(output_sentence)\n",
    "        start_losses = np.hstack(start_losses)\n",
    "        end_losses = np.hstack(end_losses)\n",
    "    return final_output, start_losses, end_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.DataFrame()\n",
    "for fold in range(1,6):\n",
    "    valid_all = pd.read_csv('train_folds_42.csv')\n",
    "    valid_all = valid_all.loc[(valid_all.kfold==(fold-1))].reset_index(drop=True)\n",
    "    valid_all['original_selected_text'] = valid_all['selected_text'].copy()\n",
    "    \n",
    "    valid_all = preprocess_im(valid_all)\n",
    "    valid_all = preprocess_all(valid_all)\n",
    "    valid_all = preprocess_repeat(valid_all)\n",
    "    valid = valid_all.copy()\n",
    "    \n",
    "    valid_data = TweetDataset(tweet=valid.text.values,\n",
    "                              sentiment=valid.sentiment.values,\n",
    "                              selected_text=valid.selected_text.values)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_data,\n",
    "        shuffle=False,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        num_workers=1\n",
    "    )\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f'Roberta_Local_{fold}.pth'))\n",
    "    model.to(device)\n",
    "    temp_valid = valid.copy()\n",
    "    best_jaccard = 0\n",
    "    # print('Epoch:', epoch+1, '|', end=' ')\n",
    "    temp_valid['pred'], temp_valid['start_loss'], temp_valid['end_loss'] = inf_fn(valid_data_loader, model, device)\n",
    "    # temp_valid.loc[temp_valid['pred'] == '', 'pred'] = temp_valid.loc[temp_valid['pred'] == '', 'text']\n",
    "    temp_valid['j_score'] = temp_valid.apply(lambda x: jaccard(x['selected_text'], x['pred']), axis=1)\n",
    "    jaccard_score = temp_valid['j_score'].mean()\n",
    "    valid_final = temp_valid.copy()\n",
    "    valid_final['j_score'] = valid_final.apply(lambda x: jaccard(x['selected_text'], x['pred']), axis=1)\n",
    "    print('fold', fold, ':', valid_final['j_score'].mean(), valid_final['start_loss'].mean(), valid_final['end_loss'].mean())\n",
    "    oof = oof.append(valid_final, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_real = pd.read_csv('train_folds_42.csv')\n",
    "all_valid = valid_real.merge(oof, how='left', left_on='textID', right_on='textID')\n",
    "\n",
    "all_valid = postprocess_im(all_valid)\n",
    "all_valid = postprocess_all(all_valid)\n",
    "all_valid = postprocess_repeat(all_valid)\n",
    "all_valid['pred'] = all_valid.pred.apply(lambda x: x.replace(\"\\*\", \"*\"))\n",
    "\n",
    "all_valid['pred'] = all_valid['pred'].apply(lambda x: x.replace('!!!!', '!!') if len(x.split())==1 else x)\n",
    "all_valid['pred'] = all_valid['pred'].apply(lambda x: x.replace('!!!', '!!') if len(x.split())==1 else x)\n",
    "all_valid['pred'] = all_valid['pred'].apply(lambda x: x.replace('....', '..') if len(x.split())==1 else x)\n",
    "all_valid['pred'] = all_valid['pred'].apply(lambda x: x.replace('...', '..') if len(x.split())==1 else x)\n",
    "\n",
    "\n",
    "all_valid.loc[all_valid['sentiment_x']=='neutral', 'pred'] = all_valid.loc[\n",
    "    all_valid['sentiment_x']=='neutral', 'text_x']\n",
    "\n",
    "all_valid['j_score_real'] = all_valid.apply(lambda x: jaccard(x['selected_text_x'], x['pred']), axis=1)\n",
    "all_valid.j_score_real.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
